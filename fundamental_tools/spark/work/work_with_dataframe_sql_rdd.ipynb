{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1af3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24510143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ROOT_PATH = os.path.join(os.getcwd(), \"..\")\n",
    "\n",
    "DATA_PATH = os.path.join(ROOT_PATH, \"data\")\n",
    "OUTPUT_PATH = os.path.join(ROOT_PATH, \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5478e",
   "metadata": {},
   "source": [
    "## Load 2 CSV file to 2 dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e10042da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Chapter: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Page: integer (nullable = true)\n",
      "\n",
      "+-------+--------------------+----+\n",
      "|Chapter|                Name|Page|\n",
      "+-------+--------------------+----+\n",
      "|      1|        Introduction|  11|\n",
      "|      2|Basic Engineering...|  19|\n",
      "|      3|Advanced Engineer...|  28|\n",
      "|      4|     Hands On Course|  60|\n",
      "|      5|        Case Studies|  62|\n",
      "|      6|Best Practices Cl...|  73|\n",
      "|      7|130+ Data Sources...|  77|\n",
      "|      8|1001 Interview Qu...|  82|\n",
      "|      9|Recommended Books...|  87|\n",
      "+-------+--------------------+----+\n",
      "\n",
      "root\n",
      " |-- Chapter: integer (nullable = true)\n",
      " |-- Section: double (nullable = true)\n",
      " |-- Section_Name: string (nullable = true)\n",
      "\n",
      "+-------+-------+--------------------+\n",
      "|Chapter|Section|        Section_Name|\n",
      "+-------+-------+--------------------+\n",
      "|      1|    1.1|What is this Cook...|\n",
      "|      1|    1.2|Data Engineer vs ...|\n",
      "|      1|    1.3|My Data Science P...|\n",
      "|      1|    1.4|  Who Companies Need|\n",
      "|      2|    2.1|       Learn To Code|\n",
      "|      2|    2.2|Get Familiar With...|\n",
      "|      2|    2.3|   Agile Development|\n",
      "|      2|    2.4|Software Engineer...|\n",
      "|      2|    2.5|Learn how a Compu...|\n",
      "|      2|    2.6|Data Network Tran...|\n",
      "|      2|    2.7|Security and Privacy|\n",
      "|      2|    2.8|               Linux|\n",
      "|      2|    2.9|              Docker|\n",
      "|      2|    2.1|           The Cloud|\n",
      "|      2|   2.11|Security Zone Design|\n",
      "|      3|    3.1|Data Science Plat...|\n",
      "|      3|    3.2|    Hadoop Platforms|\n",
      "|      3|    3.3|             Connect|\n",
      "|      3|    3.4|              Buffer|\n",
      "|      3|    3.5|Processing Framew...|\n",
      "+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-------+--------------------+\n",
      "|Chapter|Section|        Section_Name|\n",
      "+-------+-------+--------------------+\n",
      "|      5|    5.1| Data Science Airbnb|\n",
      "|      5|    5.1|Data Science Driv...|\n",
      "+-------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load book contents\n",
    "book_chapter_df = spark.read\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .csv(os.path.join(DATA_PATH, \"bookcontents.csv\"))\n",
    "\n",
    "book_chapter_df.printSchema()\n",
    "book_chapter_df.show()\n",
    "\n",
    "\n",
    "# Load section\n",
    "section_df = spark.read\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .csv(os.path.join(DATA_PATH, \"sections.csv\"))\n",
    "\n",
    "section_df.printSchema()\n",
    "section_df.show()\n",
    "\n",
    "# Test filter some data in the dataframe\n",
    "section_df.where(\"Chapter == 5 and Section == 5.1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e3697",
   "metadata": {},
   "source": [
    "## Join section with book content by chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9e74395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+-------+------------+----+\n",
      "|Chapter|Section|        Section_Name|Chapter|        Name|Page|\n",
      "+-------+-------+--------------------+-------+------------+----+\n",
      "|      1|    1.1|What is this Cook...|      1|Introduction|  11|\n",
      "|      1|    1.2|Data Engineer vs ...|      1|Introduction|  11|\n",
      "|      1|    1.3|My Data Science P...|      1|Introduction|  11|\n",
      "|      1|    1.4|  Who Companies Need|      1|Introduction|  11|\n",
      "+-------+-------+--------------------+-------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "section_df\\\n",
    "    .join(book_chapter_df, book_chapter_df.Chapter == section_df.Chapter, \"inner\")\\\n",
    "    .filter((book_chapter_df.Chapter.isin(1, 2)) & (book_chapter_df.Page == 11))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5dbee6",
   "metadata": {},
   "source": [
    "## Access and do expression on columns from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc9074df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----+-------+--------------------+\n",
      "|Chapter|                Name|Page|Section|        Section_Name|\n",
      "+-------+--------------------+----+-------+--------------------+\n",
      "|      1|        Introduction|  11|    1.1|What is this Cook...|\n",
      "|      1|        Introduction|  11|    1.2|Data Engineer vs ...|\n",
      "|      1|        Introduction|  11|    1.3|My Data Science P...|\n",
      "|      1|        Introduction|  11|    1.4|  Who Companies Need|\n",
      "|      2|Basic Engineering...|  19|    2.1|       Learn To Code|\n",
      "|      2|Basic Engineering...|  19|    2.2|Get Familiar With...|\n",
      "|      2|Basic Engineering...|  19|    2.3|   Agile Development|\n",
      "|      2|Basic Engineering...|  19|    2.4|Software Engineer...|\n",
      "|      2|Basic Engineering...|  19|    2.5|Learn how a Compu...|\n",
      "|      2|Basic Engineering...|  19|    2.6|Data Network Tran...|\n",
      "|      2|Basic Engineering...|  19|    2.7|Security and Privacy|\n",
      "|      2|Basic Engineering...|  19|    2.8|               Linux|\n",
      "|      2|Basic Engineering...|  19|    2.9|              Docker|\n",
      "|      2|Basic Engineering...|  19|    2.1|           The Cloud|\n",
      "|      2|Basic Engineering...|  19|   2.11|Security Zone Design|\n",
      "|      3|Advanced Engineer...|  28|    3.1|Data Science Plat...|\n",
      "|      3|Advanced Engineer...|  28|    3.2|    Hadoop Platforms|\n",
      "|      3|Advanced Engineer...|  28|    3.3|             Connect|\n",
      "|      3|Advanced Engineer...|  28|    3.4|              Buffer|\n",
      "|      3|Advanced Engineer...|  28|    3.5|Processing Framew...|\n",
      "+-------+--------------------+----+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-------+-----------+\n",
      "|Chapter|Section|Inc_Section|\n",
      "+-------+-------+-----------+\n",
      "|      2|    2.1|        3.1|\n",
      "|      2|    2.2|        3.2|\n",
      "|      2|    2.3|        3.3|\n",
      "|      2|    2.4|        3.4|\n",
      "|      2|    2.5|        3.5|\n",
      "|      2|    2.6|        3.6|\n",
      "|      2|    2.7|        3.7|\n",
      "|      2|    2.8|        3.8|\n",
      "|      2|    2.9|        3.9|\n",
      "|      2|    2.1|        3.1|\n",
      "|      2|   2.11|       3.11|\n",
      "+-------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book_contents_df = book_chapter_df\\\n",
    "                        .join(section_df, \"Chapter\")\n",
    "\n",
    "book_contents_df.show()\n",
    "\n",
    "book_contents_df.select(\n",
    "    book_contents_df.Chapter,\n",
    "    book_contents_df.Section,\n",
    "    (book_contents_df.Section + 1).alias(\"Inc_Section\")\n",
    ")\\\n",
    "    .where(book_contents_df.Chapter == 2)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ebd2cc",
   "metadata": {},
   "source": [
    "## Group By and Aggregation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e4e801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Chapter|count|\n",
      "+-------+-----+\n",
      "|      1|    4|\n",
      "|      6|    3|\n",
      "|      3|    8|\n",
      "|      5|   37|\n",
      "|      9|    3|\n",
      "|      4|    7|\n",
      "|      8|    1|\n",
      "|      7|   19|\n",
      "|      2|   11|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book_contents_df.groupBy(book_contents_df.Chapter).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da5f11",
   "metadata": {},
   "source": [
    "## Work with Spark SQL (We totally work directly with Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb0b8257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Chapter: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Page: integer (nullable = true)\n",
      " |-- Section: double (nullable = true)\n",
      " |-- Section_Name: string (nullable = true)\n",
      "\n",
      "+-------+------------+----+-------+--------------------+\n",
      "|Chapter|        Name|Page|Section|        Section_Name|\n",
      "+-------+------------+----+-------+--------------------+\n",
      "|      1|Introduction|  11|    1.1|What is this Cook...|\n",
      "|      1|Introduction|  11|    1.2|Data Engineer vs ...|\n",
      "|      1|Introduction|  11|    1.3|My Data Science P...|\n",
      "|      1|Introduction|  11|    1.4|  Who Companies Need|\n",
      "+-------+------------+----+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view\n",
    "book_contents_df.createOrReplaceTempView(\"book_contents_view\")\n",
    "\n",
    "# Select from view above\n",
    "view_df = spark.sql(\"\"\"SELECT * FROM book_contents_view WHERE Page = 11\"\"\")\n",
    "view_df.printSchema()\n",
    "view_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f49a4afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='book_contents_view', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef649f6",
   "metadata": {},
   "source": [
    "## Create RDD from Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "71e2858c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Chapter=1, Name='Introduction', Page=11, Section=1.1, Section_Name='What is this Cookbook')\n",
      "Row(Chapter=1, Name='Introduction', Page=11, Section=1.2, Section_Name='Data Engineer vs Data Scientist')\n",
      "Row(Chapter=1, Name='Introduction', Page=11, Section=1.3, Section_Name='My Data Science Platform Blueprint')\n",
      "Row(Chapter=1, Name='Introduction', Page=11, Section=1.4, Section_Name='Who Companies Need')\n",
      "Row(Chapter=2, Name='Basic Engineering Skills', Page=19, Section=2.1, Section_Name='Learn To Code')\n",
      "Row(Chapter=2, Name='Basic Engineering Skills', Page=19, Section=2.2, Section_Name='Get Familiar With Git')\n",
      "Row(Chapter=2, Name='Basic Engineering Skills', Page=19, Section=2.3, Section_Name='Agile Development')\n",
      "Row(Chapter=2, Name='Basic Engineering Skills', Page=19, Section=2.4, Section_Name='Software Engineering Culture')\n",
      "Row(Chapter=2, Name='Basic Engineering Skills', Page=19, Section=2.5, Section_Name='Learn how a Computer Works')\n",
      "Row(Chapter=2, Name='Basic Engineering Skills', Page=19, Section=2.6, Section_Name='Data Network Transmission')\n"
     ]
    }
   ],
   "source": [
    "book_contents_rdd = book_contents_df.rdd\n",
    "\n",
    "for row in book_contents_rdd.take(10):\n",
    "    print(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "04dab092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Chaper=1, Compound='11/Introduction')\n",
      "Row(Chaper=1, Compound='11/Introduction')\n",
      "Row(Chaper=1, Compound='11/Introduction')\n",
      "Row(Chaper=1, Compound='11/Introduction')\n",
      "Row(Chaper=2, Compound='19/Basic Engineering Skills')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import Row, StructField, StringType, StructType, IntegerType\n",
    "\n",
    "def compound_column(columns):\n",
    "    \n",
    "    dims = str(columns[2]) + \"/\" + columns[1]\n",
    "    return Row(\n",
    "        Chaper=columns[0],\n",
    "        Compound=dims\n",
    "    )\n",
    "\n",
    "splitRDD = book_contents_rdd.map(compound_column)\n",
    "\n",
    "for row in splitRDD.take(5):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6cff0f",
   "metadata": {},
   "source": [
    "## Convert RDD back to DataFrame with defined schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7ac92229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Chapter: string (nullable = false)\n",
      " |-- Compound: string (nullable = false)\n",
      "\n",
      "+-------+--------------------+\n",
      "|Chapter|            Compound|\n",
      "+-------+--------------------+\n",
      "|      1|     11/Introduction|\n",
      "|      1|     11/Introduction|\n",
      "|      1|     11/Introduction|\n",
      "|      1|     11/Introduction|\n",
      "|      2|19/Basic Engineer...|\n",
      "|      2|19/Basic Engineer...|\n",
      "|      2|19/Basic Engineer...|\n",
      "|      2|19/Basic Engineer...|\n",
      "|      2|19/Basic Engineer...|\n",
      "|      2|19/Basic Engineer...|\n",
      "|      2|19/Basic Engineer...|\n",
      "|      2|19/Basic Engineer...|\n",
      "|      2|19/Basic Engineer...|\n",
      "|      2|19/Basic Engineer...|\n",
      "|      2|19/Basic Engineer...|\n",
      "|      3|28/Advanced Engin...|\n",
      "|      3|28/Advanced Engin...|\n",
      "|      3|28/Advanced Engin...|\n",
      "|      3|28/Advanced Engin...|\n",
      "|      3|28/Advanced Engin...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Chapter\", StringType(), False),\n",
    "    StructField(\"Compound\", StringType(), False)\n",
    "])\n",
    "\n",
    "compound_df = spark.createDataFrame(splitRDD, schema)\n",
    "\n",
    "compound_df.printSchema()\n",
    "\n",
    "compound_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67993fc5",
   "metadata": {},
   "source": [
    "## Spark counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f58d977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1.1,What is this Cookbook\n",
      "1,1.2,Data Engineer vs Data Scientist\n",
      "1,1.3,My Data Science Platform Blueprint\n",
      "1,1.4,Who Companies Need\n",
      "2,2.1,Learn To Code\n",
      "====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Open file text in Spark as RDD format\n",
    "section_rdd = sc.textFile(os.path.join(DATA_PATH, \"sections_wordcount.csv\"))\n",
    "\n",
    "# Let view some rows -> this is a text\n",
    "for row in section_rdd.take(5):\n",
    "    print(row)\n",
    "    \n",
    "print(\"====\\n\")\n",
    "    \n",
    "# Map through each row then split by delimiter is ,\n",
    "section_split_rdd = section_rdd\\\n",
    "                        .map(lambda line: line.split(\",\"))\\\n",
    "                        .map(lambda elements: elements[2].lower())\\\n",
    "                        .flatMap(lambda sentence: sentence.split(\" \"))\\\n",
    "                        .map(lambda word: (word, 1))\\\n",
    "                        .reduceByKey(lambda x, y: x+y)\\\n",
    "                        .sortByKey(False)\n",
    "\n",
    "# section_split_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f6b85c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Word: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "+-----------+-----+\n",
      "|       Word|Count|\n",
      "+-----------+-----+\n",
      "|       data|   48|\n",
      "|    science|   40|\n",
      "|        and|    9|\n",
      "|         to|    4|\n",
      "|     apache|    3|\n",
      "|   platform|    3|\n",
      "| processing|    3|\n",
      "|          a|    3|\n",
      "|    courses|    2|\n",
      "|    twitter|    2|\n",
      "|       with|    2|\n",
      "|        the|    2|\n",
      "|     amazon|    2|\n",
      "|      learn|    2|\n",
      "|development|    2|\n",
      "|     google|    2|\n",
      "|       from|    2|\n",
      "|       nifi|    2|\n",
      "|       what|    2|\n",
      "|   zeppelin|    2|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_count_schema = StructType([\n",
    "    StructField(\"Word\", StringType()),\n",
    "    StructField(\"Count\", IntegerType())\n",
    "])\n",
    "\n",
    "word_count_df = spark.createDataFrame(section_split_rdd, word_count_schema)\n",
    "\n",
    "word_count_df.printSchema()\n",
    "word_count_df.orderBy(word_count_df.Count.desc()).show()\n",
    "\n",
    "word_count_df.write.mode(\"overwrite\").csv(os.path.join(OUTPUT_PATH, \"word_count.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
